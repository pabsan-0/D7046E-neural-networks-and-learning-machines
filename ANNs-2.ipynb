{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"colab":{"name":"Submitted-ANN-Exercise-2.ipynb","provenance":[],"collapsed_sections":["EceZkkDAT-uB"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"GsTd9bJYT-t-"},"source":["# Neural Networks and Learning Machines\n","\n","## ANN Exercise 2 - Natural Language Processing (NLP)\n","\n","Natural Language Processing (NLP) is a set of techniques and algorithms that use computers for analyzing natural human language.  NLP can be used to solve a variety of problems, for example:\n","\n","* Subject Matter Identification - What are the topics, people, companies or places discussed in this text?\n","* Sentiment Analysis - Does this text convey a positive, negative or neutral feeling about an entity or subject matter?\n","* Machine Translation - Convert the input from one language to another, for example, from English to French\n","* Text-to-Speech - Convert the spoken input into its written form\n","* Understanding and Interpretation - What information will answer a specific question?\n","\n","This exercise will teach you the basics of NLP using neural networks, focusing on text classification.\n","First you will learn a few common ways to encode/embed text as vectors/matrices such that a neural network can use it as an input.\n","Then you'll use those embeddings in order to embed a dataset of texts and classify them.\n","Finally you'll learn how to use some more advanced neural networks models in order to generate text.\n","\n","Before you get started you'll also need to download the file 'wiki.simple.vec' from Canvas, which contains word embeddings that will be used in this exercise. Do **NOT** include 'wiki.simple.vec' in your submission of this exercise.\n","\n","### Literature\n","\n","The following sections in the [course book](https://www.deeplearningbook.org/) are relevant for this exercise.\n","* 12.4 - Natural Language Processing\n","   - (more subsection details as far from every subsection of NLP is useful to the course)\n","\n","Optional reading for more details on RNNs:\n","* 10 - Sequence Modeling: Recurrentand Recursive Nets"]},{"cell_type":"code","metadata":{"id":"qDhOgN3jT-t-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606905899335,"user_tz":-60,"elapsed":9604,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"c3943f85-0945-4bf9-b170-9237a8f62c44"},"source":["!pip install --upgrade torchtext\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","from torchvision import datasets, transforms\n","from torchtext.datasets import AG_NEWS\n","import torchtext\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import gensim"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting torchtext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/23/8499af6d9c22b29b01f66a2c11d38ce71cd1cafa2655913c29818ed4a00f/torchtext-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (6.9MB)\n","\u001b[K     |████████████████████████████████| 6.9MB 4.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.8)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.11.8)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n","Installing collected packages: torchtext\n","  Found existing installation: torchtext 0.3.1\n","    Uninstalling torchtext-0.3.1:\n","      Successfully uninstalled torchtext-0.3.1\n","Successfully installed torchtext-0.8.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eYRytdSMT-t-"},"source":["## Turning sentences into matrices\n","\n","As you know a neural network can only process numbers, so anything we want to process with a neural networks needs to be encoded (embedded) into number, aka vectors.\n","We've technically already done this for images in exercise 0 and ANN1 where we encoded the images of handwritten digits as numbers representing the grayscale value of each pixel.\n","For images this process was intuitive as we're already used to representing pixels as numbers, but how do you do this for text?<img src=\"attachment:image.png\" width=\"600\">\n","\n","There are many different ways to encode text for neural network applications. You can replace each letter by their position in the alphabet, you can replace each word by their position in some dictionary, you can create complex vectors that distribute words in some N-dimensional space, etc, etc.\n","Most encodings of text is done on the word level, since we know that we want to use words anyways and learning to create words from letters turns out to be quite challenging ('runes' and 'nurse' contain the exact same letters but mean very different things).\n","\n","In this exercise we will learn to create and use two types of embeddings:\n","* Bag of Words (BoW)\n","* Word2Vec (Skip-gram)\n","\n","We will also learn about another Word2Vec implementation called Continuous Bag of Words (CBoW), and you may implement this as well as an optional task.\n"," \n","Finally we will also learn to use pretrained embeddings, where you simply use embeddings created by someone else.\n","\n","Before we can start with each embedding, we have to discuss the so-called preprocessing pipeline.\n","The preprocessing pipeline is simply all the step we take from raw data (text) to the input of our neural networks.\n","There are many ways to form such a pipeline, with many different optional steps to add on, but generally it consists of some or all of the following parts:\n","* Cleaning - Removing unnecessary/unimportant parts of the text (like making all letters lowercase, removing special characters, etc)\n","   - Example: \"Does Michael like 99 cats?\" $\\rightarrow$ \"does michael like 99 cats\"\n","* Tokenization - Splitting the text into its constituent parts (like words, letters, and/or symbols depending on what embedding you use)\n","   - Example: \"does michael like 99 cats\" $\\rightarrow$ \\[\"does\", \"michael\", \"like\", \"99\", \"cats\"\\]\n","* Stemming/lemmatization - Switch all words into their baseform (like removing plural, changing verbs to their root form, etc)\n","   - Example: \\[\"does*\", \"michael\", \"like\", \"99\", \"cats\"\\] $\\rightarrow$ \\[\"do\", \"michael\", \"like\", \"99\", \"cat\"\\]\n","* Replace rare/special tokens - Switch some rare (only shows up once or twice in the dataset) or special (names, numbers, etc) tokens for tags\n","   - Example: \\[\"do\", \"michael\", \"like\", \"99\", \"cat\"\\] $\\rightarrow$ \\[\"do\", \"\\<NAME\\>\", \"like\", \"\\<NUMBER\\>\", \"cat\"\\]\n","* Indexing - Create a token to number mapping and replace each token by its number\n","   - Example: \\[\"do\", \"\\<NAME\\>\", \"like\", \"\\<NUMBER\\>\", \"cat\"\\] $\\rightarrow$ \\[454, 3, 872, 2, 234\\]\n","* Embedding - Use an embedder to get a vector to use as input to the machine learning model.\n","\n","This might seem overwhelming but there are a lot of toolkits that will help you with these steps.\n","This might also seem like we're getting rid of a lot of useful data, and that's true but often having well-formatted data is better than having more data.\n","In the end it depends from problem to problem which preprocessing steps are best.\n","For this exercise you will only need to think of tokenizing, indexing, and embedding.\n","The AG News dataset have already been tokenized and indexed, but you will need to create a tokeizer and indexer for the \"cat on the mat\" dataset.\n","\n","### Datasets\n","\n","In this exercise we will use two text datasets the \"cat on the mat\"-dataset and the [AG News dataset](https://www.kaggle.com/amananandrai/ag-news-classification-dataset). The code for creating/downloading both datasets are provided below. The former dataset is a small dummy dataset we will use to visulize and understand the methods. AG News is a large dataset of excerpts from news articles from a few different topics that can be used to train, for example, a news classifier."]},{"cell_type":"markdown","metadata":{"id":"YkwcE57LT-t-"},"source":["#### \"Cat on the mat\"-dataset"]},{"cell_type":"code","metadata":{"id":"Eq-DokuYT-t-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606905899336,"user_tz":-60,"elapsed":9594,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"ee6ba2ca-0a18-4a60-af2c-bb8e2adde1d2"},"source":["import itertools\n","\n","animals = ['cat','dog','goat','elephant','eagle','zebra','rhino', 'hippo']\n","actions = ['sat','stood','jumped','slept']\n","furniture = ['mat','rug','sofa','bed']\n","\n","# Generate all combinations of animal, action and furniture\n","animal_corpus = [f'the {x[0]} {x[1]} on the {x[2]}' for x in itertools.product(animals, actions, furniture)]\n","vocabulary_size = len(animals) + len(actions) + len(furniture) + 2\n","\n","print(f'There are {len(animal_corpus)} sentences in the corpus, with a vocabulary of {vocabulary_size} words')\n","print(f'The first sentence in the corpus is \"{animal_corpus[0]}\"')\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["There are 128 sentences in the corpus, with a vocabulary of 18 words\n","The first sentence in the corpus is \"the cat sat on the mat\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SKHXLWM2YiLw","executionInfo":{"status":"ok","timestamp":1606905899336,"user_tz":-60,"elapsed":9586,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"f27df70b-1f94-4ed7-b5a5-462b9872be8f"},"source":["# Returns all possible combinations of the argument\n","[i for i in itertools.product([1],[2,3,4])]"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, 2), (1, 3), (1, 4)]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"7jFUjHpIT-t_"},"source":["#### AG News dataset"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"sHb73eOtT-t_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606905920549,"user_tz":-60,"elapsed":30792,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"b7ef87eb-6ac2-4299-9acb-4a294dd0792c"},"source":["## Download,tokenize, and index the AG News dataset\n","# This is done in a separate cell as indexing takes some time\n","\n","ag_train, ag_test = AG_NEWS(ngrams=1, vocab=None)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["ag_news_csv.tar.gz: 11.8MB [00:00, 25.3MB/s]\n","120000lines [00:05, 23487.61lines/s]\n","120000lines [00:11, 10561.95lines/s]\n","7600lines [00:00, 10756.24lines/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"5PJ3cv7GT-uA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606905920550,"user_tz":-60,"elapsed":30785,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"ed2a35df-efc1-41d5-b3b4-6e7975221dae"},"source":["# Our labels are numbers, here's what type of article they represent\n","ag_labels = {\n","    0 : 'World',\n","    1 : 'Sports',\n","    2 : 'Business',\n","    3 : 'Sci/Tec'\n","}\n","\n","# vocab is a class that will give us the index for any given word/token (vocab['hi'] = <some number>)\n","vocab = ag_train.get_vocab()\n","\n","# nr2word is a list that contains the actual tokens for a given index (nr2word[5678] = <some word/token>)\n","nr2word = vocab.itos\n","\n","# Let's make a function that takes an entire tensor and returns the string version of that tensor\n","def tensor2text(tensor):\n","    return ' '.join([nr2word[index] for index in tensor])\n","\n","# And a function to turn text into the indexed version (though it won't work with special characters)\n","def text2tensor(text):\n","    tensor = torch.Tensor([vocab[token] for token in text.split(' ')])\n","    return tensor.int()\n","\n","# And let's see what the dataset actually contains\n","ag_train_len = len(ag_train) #Get the number of entries in the AG_NEWS dataset\n","\n","print(\n","    f'ag_train contains {len(ag_train)} labelled text snippets '\n","    f'and has a vocabulary size of {len(vocab)}\\n'\n","    f'On index 0, ag_train contains:\\n{ag_train[0]}\\n'\n","    f'which has the label {ag_train[0][0]} that translates into \"{ag_labels[ag_train[0][0]]}\" '\n","    f'and the text snippet itself translates into:\\n{tensor2text(ag_train[0][1])}'\n",")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["ag_train contains 120000 labelled text snippets and has a vocabulary size of 95812\n","On index 0, ag_train contains:\n","(2, tensor([  432,   426,     2,  1606, 14839,   114,    67,     3,   849,    14,\n","           28,    15,    28,    16, 50726,     4,   432,   375,    17,    10,\n","        67508,     7, 52259,     4,    43,  4010,   784,   326,     2]))\n","which has the label 2 that translates into \"Business\" and the text snippet itself translates into:\n","wall st . bears claw back into the black ( reuters ) reuters - short-sellers , wall street ' s dwindling\\band of ultra-cynics , are seeing green again .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HkZqg_alT-uA"},"source":["## Tokenizing and Indexing\n","This new corpus of texts needs to be tokenized and indexed before we can start using it.\n","The tokenizer only needs to be able to handle this simple dataset, so it can be fairly simple.\n","\n","**Exercise:** Write a tokenizer and an vocabulary class with a function that will give you the index of a token, and a function that will give you the token at a certain index.\n"]},{"cell_type":"code","metadata":{"id":"vHuW7rqrT-uA","executionInfo":{"status":"ok","timestamp":1606905920552,"user_tz":-60,"elapsed":30786,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}}},"source":["def simple_tokenizer(text):\n","    '''Tokenizes a given string'''\n","    return text.split(' ')\n","\n","class SimpleVocabulary():\n","    def __init__(self, corpus):\n","        # concatenate all sentences into one, if there are any\n","        if isinstance(corpus, list):\n","            corpus = ' '.join(corpus)\n","\n","        # Give an index to each unique word, kill duplicates\n","        words =  enumerate(list(set(simple_tokenizer(corpus))))\n","\n","        # Build a dict containing unique words with indices\n","        self.token2index = {key.lower(): val for (val, key) in words}\n","        self.index2token = list(set(simple_tokenizer(corpus)))\n","\n","    def get_index(self, token):\n","        return self.token2index[str(token).lower()]\n","    \n","    def get_token(self, index):\n","        return self.index2token[int(index)]\n","\n","    def __len__(self):\n","        return len(self.token2index)\n","\n","\n","def index_sequence(sequence, vocabulary):\n","    if isinstance(sequence, str):\n","        sequence = simple_tokenizer(sequence)\n","    return [vocabulary.get_index(token) for token in sequence]"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a2s650UvT-uA"},"source":["**Check your solution**: The code cell below will try to tokenize and index a few sentences"]},{"cell_type":"code","metadata":{"id":"0UjrN3swT-uA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606905920942,"user_tz":-60,"elapsed":31169,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"ac6585ce-bb44-42cf-bbbe-a36996cf4322"},"source":["animal_vocab = SimpleVocabulary(animal_corpus)\n","test_sequence = 'the cat stood on the mat'\n","tokenized_sequence = simple_tokenizer(test_sequence)\n","print(tokenized_sequence)\n","indexed_sequence = index_sequence(tokenized_sequence, animal_vocab)\n","print(indexed_sequence)\n","assert len(tokenized_sequence) == 6, 'The tokenized sequence should contain all words of the original sequence'\n","assert len(tokenized_sequence) == len(indexed_sequence), 'The indexed sequence should be as long as the tokenized sequence'\n","assert indexed_sequence[0] == indexed_sequence[4], 'The index of the same word should be the same'\n","assert len(animal_vocab) >= 18, 'There should be at least 18 words in the vocabulary (more if you have a out-of-vocabulary token/index)'\n","print('simple_tokenizer and SimpleVocabulary seems to be properly implemented!')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["['the', 'cat', 'stood', 'on', 'the', 'mat']\n","[2, 10, 14, 16, 2, 6]\n","simple_tokenizer and SimpleVocabulary seems to be properly implemented!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5Vopnav4T-uB"},"source":["## Bag of Words (BoW)\n","\n","Bag of Words is often seen as the default way to embed text as it is very easy to do.\n","In bag of words each text is represented as a N-dimensional vector, where N is the number of words in the vocabulary.\n","Each word in the vocabulary is given an index and each text is represented by a vector where the index of each word contains the number of occurences of this word in the text.\n","For example if we have the vocabulary \\[\"the\", \"cat\", \"dog\", \"sat\", \"on\", \"mat\", \"sofa\"\\] then the text \"the cat sat on the mat\" would be represented by the BoW vector \\[2, 1, 0, 1, 1, 1, 0\\]\n","Simply put, each text is represented by the by stuffing all of its words into a bag, without care for their order.\n","While getting rid of the order of words might seem distastrous (\"I do like candy\" and \"do I like candy\" doesn't mean the same thing), this simple approach can still be enough to tackle many problems.\n","<img src=\"attachment:image.png\" width=\"600\">\n","\n","**Exercise**: Make a BoW embedder that takes the tensor (index vector) representation of a text, the length of the vocabulary, and returns a Bag of Words representation."]},{"cell_type":"code","metadata":{"id":"BazX9nWJT-uB","executionInfo":{"status":"ok","timestamp":1606905920944,"user_tz":-60,"elapsed":31169,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}}},"source":["def bow_embedder(tensor, vocab_length):\n","\n","    tensor = tensor.long()\n","    embedding = torch.zeros(vocab_length)\n","    unique, counts =  np.unique(tensor, return_counts=True)\n","\n","    for i, j in zip(unique, counts):\n","        embedding[i] = j\n","\n","    return embedding\n","\n","\n","def bow_embedder_that_does_not_work(tensor, vocab_length):\n","    ''' HEADS UP: this utterly wrong implementation of the BoW bypasses the\n","    provided checking snippet '''\n","\n","    unique, counts = np.unique(tensor, return_counts=True)\n","    counts = np.append(counts, np.zeros(vocab_length - len(counts)))\n","    return torch.tensor(counts)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GF4jC0xKT-uB"},"source":["**Check your solution**: The code cell below will try to embed a few sentences using your embedder and will check that the embeddings seem correct."]},{"cell_type":"code","metadata":{"id":"IzEbfIeIT-uB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606905922755,"user_tz":-60,"elapsed":32973,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"25baaa0a-3034-4e50-8d44-f86ac7d1eb4d"},"source":["vocab_length = len(vocab)\n","\n","sentence_1 = 'the cat dances well for a little while'\n","sentence_2 = 'for a while the little cat dances well'\n","sentence_3 = 'the dog sits for a while'\n","sentence_4 = 'the cat and the dog'\n","\n","sentence_1_indexed = text2tensor(sentence_1)\n","sentence_2_indexed = text2tensor(sentence_2)\n","sentence_3_indexed = text2tensor(sentence_3)\n","sentence_4_indexed = text2tensor(sentence_4)\n","\n","sentence_1_bow = bow_embedder(sentence_1_indexed, vocab_length)\n","sentence_2_bow = bow_embedder(sentence_2_indexed, vocab_length)\n","sentence_3_bow = bow_embedder(sentence_3_indexed, vocab_length)\n","sentence_4_bow = bow_embedder(sentence_4_indexed, vocab_length)\n","\n","# Assert statements are mainly used to test code\n","# You give them a statement that should be True and they throw an error if it doesn't\n","assert isinstance(sentence_1_bow, torch.Tensor), 'The BoW embedding should be a tensor'\n","assert len(sentence_1_bow) == vocab_length, 'The length of the BoW embedding should be the same as the vocabulary'\n","assert torch.all(sentence_1_bow == sentence_2_bow), 'Texts with the exact same words should have the same BoW embedding'\n","assert torch.any(sentence_1_bow != sentence_3_bow), 'Texts with different words should have different BoW embeddings'\n","assert sum(sentence_1_bow) == len(sentence_1_indexed), 'The sum of the BoW embedding should be the number of words in the text'\n","assert max(sentence_4_bow) == 2, 'A BoW embeddings should count the number of occurances of each word'\n","\n","print('The bow_embedder seems to be properly implemented!')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["The bow_embedder seems to be properly implemented!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wHKrTdH2T-uB"},"source":["#### Visualizing BoW\n","\n","Before we use our embedder to train an actual neural network on the AG News dataset, let's see what it looks like using our smaller \"cat on the mat\"-dataset."]},{"cell_type":"code","metadata":{"id":"LPvOjb5fT-uB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606905922756,"user_tz":-60,"elapsed":32967,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"190b6b85-303b-4e89-8085-df7b5e60bd95"},"source":["## Let's take a sentence and create the BoW representation for it\n","\n","animal_vocab = SimpleVocabulary(animal_corpus)                                    # Create vocabulary\n","test_sequence = 'the cat stood on the mat'                                        # Create the sentence\n","tokenized_sequence = simple_tokenizer(test_sequence)                              # Tokenize the sentence\n","indexed_sequence = torch.Tensor(index_sequence(tokenized_sequence, animal_vocab)) # Index the sentence and turn it into a tensor\n","bow_sequence = bow_embedder(indexed_sequence, len(animal_vocab))                  # Create the BoW embedding of the sentence\n","\n","print(f'The sentence \"{test_sequence}\" was embedded with the BoW representation: \\n{bow_sequence}\\n')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["The sentence \"the cat stood on the mat\" was embedded with the BoW representation: \n","tensor([0., 0., 2., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.])\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qLR4A_zyT-uB"},"source":["### Training a news text classifier\n","\n","So now that we have a way to embed our sentences, let's use that to train a classifier on the AG News dataset.\n","\n","**Exercise**: Create a neural network and the train and test loops, and get at least 80% accuracy on the test set. \n","\n"]},{"cell_type":"code","metadata":{"id":"LlS24re1T-uB","executionInfo":{"status":"ok","timestamp":1606905922757,"user_tz":-60,"elapsed":32966,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}}},"source":["# STUFF I WILL ONLY RUN ONCE\n","\n","vocab = ag_train.get_vocab()\n","\n","# We create our own dataset to load the BoW embedded texts\n","class BoWDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, embedder):\n","        self.dataset = dataset\n","        self.embedder = embedder\n","        \n","    def __getitem__(self, index):\n","        return self.dataset[index][0], self.embedder(self.dataset[index][1], len(vocab))\n","  \n","    def __len__(self):\n","        return len(self.dataset)\n","\n","\n","bow_train_data = BoWDataset(ag_train, bow_embedder)\n","bow_test_data = BoWDataset(ag_test, bow_embedder)\n","bow_trainloader = DataLoader(bow_train_data, batch_size=1000, shuffle=True)\n","bow_testloader = DataLoader(bow_test_data, batch_size=1000, shuffle=False)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"RROGtr5wT-uB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606905937605,"user_tz":-60,"elapsed":2939,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"9bc26a51-64a4-4c99-93bc-5248a9fbb920"},"source":["## CREATE A NEURAL NETWORK TO TRAIN\n","\n","network = nn.Sequential( \n","    nn.Linear(len(vocab), 100),\n","    nn.ReLU(),\n","    nn.Linear(100, 4)\n","    )\n","\n","optimizer = torch.optim.Adam(network.parameters(), lr = 0.05)\n","loss_function = nn.CrossEntropyLoss() \n","\n","epochs = 1 # The dataset is large so one epoch should do for our purpose (and anything more would take forever)\n","\n","for epoch in range(epochs):\n","    for batch_nr, (labels, data) in enumerate(bow_trainloader):\n","             \n","        # Make a prediction and compute its loss\n","        prediction = network(data.float())\n","\n","        print(prediction)\n","\n","        print()\n","        print(labels)\n","        break\n","\n","\n","        loss = loss_function(prediction, labels)\n","\n","\n","        # Upgrade weights\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        \n","        print(f'\\rEpoch {epoch+1} [{batch_nr+1}/{len(bow_trainloader)}] - Loss: {loss}',\n","            end='')\n","\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["tensor([[-0.0835, -0.0943, -0.0006, -0.0203],\n","        [-0.0757, -0.0976, -0.0014, -0.0219],\n","        [-0.0733, -0.0948,  0.0031, -0.0194],\n","        ...,\n","        [-0.0805, -0.0960, -0.0063, -0.0237],\n","        [-0.0780, -0.0931,  0.0057, -0.0279],\n","        [-0.0794, -0.0955, -0.0020, -0.0174]], grad_fn=<AddmmBackward>)\n","\n","tensor([1, 3, 2, 1, 3, 2, 1, 0, 2, 2, 3, 1, 0, 1, 2, 0, 2, 3, 0, 0, 2, 2, 3, 1,\n","        3, 1, 3, 3, 2, 2, 0, 3, 2, 2, 3, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 3,\n","        3, 1, 1, 2, 1, 0, 1, 0, 0, 0, 3, 0, 3, 3, 0, 1, 0, 1, 1, 3, 2, 1, 3, 3,\n","        0, 1, 3, 2, 2, 0, 0, 3, 0, 1, 3, 1, 0, 0, 0, 0, 2, 2, 0, 3, 0, 1, 0, 1,\n","        2, 0, 3, 3, 2, 0, 3, 1, 1, 1, 3, 1, 2, 3, 1, 0, 0, 2, 2, 3, 0, 1, 0, 3,\n","        3, 1, 2, 2, 1, 1, 0, 2, 1, 0, 3, 0, 0, 2, 1, 0, 1, 0, 1, 0, 3, 2, 0, 1,\n","        0, 1, 1, 0, 1, 2, 3, 2, 2, 0, 3, 0, 3, 1, 2, 3, 1, 1, 0, 1, 2, 3, 1, 3,\n","        0, 1, 1, 3, 2, 0, 3, 0, 3, 3, 3, 1, 2, 0, 3, 3, 3, 3, 2, 0, 2, 2, 2, 2,\n","        1, 1, 3, 3, 2, 2, 3, 1, 1, 0, 2, 2, 3, 3, 1, 0, 0, 2, 2, 1, 3, 2, 2, 2,\n","        2, 2, 0, 2, 3, 1, 3, 2, 2, 3, 0, 0, 3, 1, 0, 0, 1, 0, 0, 1, 3, 0, 3, 3,\n","        1, 2, 3, 1, 1, 2, 2, 2, 3, 3, 2, 1, 0, 2, 2, 3, 3, 0, 3, 3, 3, 0, 1, 0,\n","        0, 3, 1, 3, 1, 3, 0, 3, 1, 1, 1, 3, 3, 1, 2, 0, 0, 2, 3, 2, 1, 3, 1, 2,\n","        0, 1, 3, 0, 1, 3, 1, 0, 3, 2, 0, 1, 3, 3, 0, 2, 0, 1, 0, 1, 2, 2, 0, 1,\n","        3, 2, 3, 2, 2, 0, 2, 2, 3, 0, 3, 3, 2, 1, 3, 0, 3, 0, 0, 0, 0, 1, 2, 3,\n","        1, 2, 3, 1, 1, 1, 3, 0, 1, 3, 0, 3, 2, 0, 1, 3, 0, 0, 3, 0, 2, 1, 2, 3,\n","        2, 1, 1, 3, 2, 0, 0, 3, 0, 2, 1, 0, 0, 0, 3, 1, 0, 3, 3, 3, 2, 1, 0, 0,\n","        3, 3, 1, 0, 2, 3, 2, 0, 0, 1, 2, 3, 1, 2, 3, 2, 0, 0, 1, 1, 0, 0, 3, 1,\n","        1, 1, 3, 0, 2, 3, 1, 0, 2, 3, 1, 3, 3, 2, 3, 3, 1, 0, 3, 2, 1, 1, 0, 1,\n","        3, 3, 0, 0, 1, 2, 0, 3, 3, 0, 1, 3, 0, 2, 1, 2, 0, 2, 2, 3, 1, 3, 2, 0,\n","        3, 1, 2, 3, 3, 1, 3, 1, 1, 1, 2, 2, 3, 3, 0, 2, 2, 2, 0, 1, 0, 0, 3, 3,\n","        0, 2, 2, 1, 3, 0, 0, 0, 1, 0, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 0, 0, 2, 3,\n","        2, 1, 3, 2, 2, 3, 0, 0, 3, 2, 3, 2, 3, 0, 3, 2, 1, 1, 0, 2, 2, 2, 0, 1,\n","        2, 1, 1, 3, 3, 3, 3, 3, 3, 1, 0, 2, 0, 2, 2, 1, 1, 0, 0, 3, 3, 1, 3, 2,\n","        0, 1, 3, 0, 3, 0, 1, 1, 1, 0, 1, 3, 2, 3, 2, 3, 0, 0, 1, 0, 1, 3, 3, 0,\n","        1, 3, 3, 3, 0, 3, 1, 1, 2, 0, 0, 2, 0, 3, 1, 2, 1, 2, 0, 2, 0, 0, 3, 0,\n","        1, 1, 3, 0, 2, 1, 2, 3, 3, 2, 3, 0, 1, 0, 0, 2, 0, 1, 3, 0, 2, 3, 0, 0,\n","        0, 1, 3, 3, 1, 3, 0, 1, 3, 3, 1, 3, 3, 2, 2, 1, 2, 2, 1, 3, 1, 1, 0, 3,\n","        2, 0, 2, 0, 3, 0, 3, 2, 0, 0, 1, 2, 3, 1, 3, 3, 1, 1, 2, 0, 1, 0, 0, 1,\n","        3, 3, 3, 1, 3, 0, 1, 2, 2, 3, 2, 3, 0, 3, 2, 1, 3, 3, 1, 3, 2, 3, 2, 2,\n","        1, 3, 1, 2, 3, 0, 3, 1, 0, 3, 2, 3, 3, 3, 1, 1, 0, 2, 3, 1, 2, 3, 3, 0,\n","        0, 2, 1, 2, 3, 3, 2, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 2, 3, 1, 2, 1, 0, 0,\n","        1, 2, 0, 0, 0, 1, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 3, 0, 0, 1, 2, 1, 3, 1,\n","        0, 0, 0, 1, 3, 1, 2, 1, 0, 1, 3, 3, 1, 3, 2, 3, 3, 2, 0, 1, 1, 3, 2, 0,\n","        2, 1, 1, 2, 2, 0, 2, 0, 1, 3, 2, 3, 1, 2, 0, 3, 2, 2, 3, 3, 0, 2, 0, 0,\n","        1, 2, 1, 2, 0, 1, 0, 2, 3, 2, 3, 1, 2, 3, 3, 3, 0, 2, 1, 1, 3, 1, 2, 1,\n","        0, 2, 0, 3, 1, 2, 0, 2, 3, 3, 3, 0, 3, 3, 3, 3, 0, 2, 2, 1, 1, 0, 1, 0,\n","        3, 0, 0, 2, 2, 3, 2, 1, 0, 1, 1, 2, 2, 1, 0, 1, 3, 2, 2, 3, 3, 0, 3, 3,\n","        3, 2, 0, 0, 1, 1, 0, 2, 3, 0, 3, 2, 3, 3, 0, 0, 2, 1, 1, 3, 2, 0, 1, 3,\n","        0, 1, 3, 3, 2, 0, 0, 2, 2, 2, 0, 2, 1, 1, 1, 1, 3, 1, 0, 0, 1, 0, 3, 0,\n","        3, 3, 3, 1, 0, 3, 3, 2, 0, 0, 3, 2, 0, 1, 3, 1, 3, 1, 3, 0, 1, 3, 3, 1,\n","        0, 2, 1, 3, 0, 2, 3, 1, 0, 3, 3, 1, 2, 0, 1, 0, 3, 2, 1, 2, 0, 0, 2, 2,\n","        2, 3, 1, 1, 3, 3, 1, 0, 3, 1, 3, 2, 3, 0, 3, 0])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hgwJho57T-uB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606330200223,"user_tz":-60,"elapsed":9610,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"47ee95e1-9f15-46c6-8b27-3d489ffd3025"},"source":["### Test set accuracy\n","\n","with torch.no_grad():\n","    matches = 0\n","    datapoints = 0\n","\n","    for batch_nr, (labels, data) in enumerate(bow_testloader):\n","        prediction = network(data.float())\n","        matches += sum([np.argmax(prediction[i].detach().numpy()) == labels[i] for i in range(len(labels))])\n","        datapoints += len(labels)\n","  \n","    try:\n","        print(f'The accuracy of the network on test set is {matches/datapoints}.')\n","    except:\n","        print('Math error on matches/datapoints.')\n","\n","\n","        '''\n","        print(max(labels))\n","        print([i for i in labels[0:2]])\n","        print([np.argmax(i) for i in labels[0:2]])\n","        print([i for i in prediction[0:2]])\n","        print([np.argmax(i) for i in prediction[0:2]])\n","        break\n","        '''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The accuracy of the network on test set is 0.9194737076759338.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bG2wZjV7T-uB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606401620965,"user_tz":-60,"elapsed":569,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"67e956df-364f-4772-c842-30184507cfd7"},"source":["# Let's print a sentence and predict it's category\n","try:\n","    sentence_index = 100\n","    prediction = torch.argmax(network(torch.unsqueeze(bow_embedder(ag_train[sentence_index][1], len(vocab)).float(), dim=0))).item()\n","\n","    print(\n","        f'The network predicted that \\n\"{tensor2text(ag_train[sentence_index][1])}\"\\n should be in the category {ag_labels[prediction]}'\n","    )\n","except:\n","    print('We did not train the network yet during this session :)')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["We did not train the network yet during this session :)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XrtcOKEtT-uB"},"source":["## Word2Vec\n","\n","As you might already have thought BoW embeddings have a few problems.\n","Firstly, it diregards the order of words which, as previously stated, can be quite important.\n","Secondly, the size of the embedding grows larger with the size of the vocabulary.\n","While the second issue can be solved by limiting the vocabulary to the most common/important words, this remains a fairly large vector with very few non-zero elements (a sparse vector).\n","\n","One commonly used alternative that fixes both of these issues is the so-called Word2Vec embeddings.\n","Word2Vec can be summarized as using machine learning to learn non-sparse (most, if not all, entries are non-zero) word embeddings with fewer dimensions (typically between 100 and 1000). A text is then represented as a matrix where the first row (or column depending on how you use it) represents the first word and so on.\n","<img src=\"attachment:image.png\" width=\"700\">\n","\n","So how do you learn an embedding?\n","Again, there are many ways to do this, but the most common Word2Vec methods use the idea that a word is defined by the company it keeps.\n","That is, an embedding of a word is good if you can either use it to predict which the surrounding words are or if it can be predicted using the surrounding words.\n","So you either train a model to predict the surrounding words from the central word, or train it to predict a word from the surrounding ones.\n","These two methods are called Skip-Gram and Continuous Bag of Words (CBoW) respectively and are illustrated by the images below.\n","\n","<h1><center>Skip-gram</center></h1>| <h2><center>CBoW</center></h2>\n","- | -\n","<img src=\"attachment:image-3.png\" width=\"300\"> | <img src=\"attachment:image-2.png\" width=\"300\">\n","\n","The logic behind these methods can be better understood by imagining synonyms.\n","If we're creating embeddings (codes) to represent different words, it makes sense that similar words should have similar embeddings.\n","Since synonymous words would have many similar surrounding words in many sentences, telling the difference between the two would be difficult, and thus giving them similar embeddings would lead to a lesser error.\n","However, words that have slightly different meaning cannot have the same embeddings, because that would make predicting words from that embedding more difficult.\n","\n","While training useful such embeddings for every single words takes huge datasets of texts, this only has to be done once and then those embeddings can be used for many different applications.\n","Due to this time consuming training we will only train a very small sample of word embeddings to learn how it's done and better understand them.\n","Afterwards we will use pretrained embeddings to do some actual prediction.\n","\n","With this in mind we will use the \"cat on the mat\" dataset to train our embeddings.\n","Our goal is to create embeddings where each group of words (animals, furniture, actions, and other) are given distinct groups in the encoding space."]},{"cell_type":"markdown","metadata":{"id":"lNQV8DO1T-uB"},"source":["### Skip-gram embedding\n","\n","In Skip-gram embeddings we want to predict the surrounding words from a given one.\n","Rather than for a given word predicting all the surrounding words at once, we'll create context-target pairs.\n","We decide on a window size, and create a context target pair with the central word being the context and each word in the window of surrounding words will be our target. \n","<img src=\"attachment:image.png\" width=\"600\">\n","\n","The goal of the training is to predict the target word given the context word.\n","We provide the function for creating context-target pairs below, so you don't need to implement this yourself.\n","\n","We'll use these context-target pairs to train a neural network to predict the target word and, hopefully, during this training the hidden layer of the neural network will become the embedding of our context word.\n","So in order to get the embedding after training, we simply input a word and extract the values of the hidden layer of the network"]},{"cell_type":"code","metadata":{"id":"JEcL-op1T-uB","executionInfo":{"status":"ok","timestamp":1606898290565,"user_tz":-60,"elapsed":659,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}}},"source":["## Hyperparameters for Skip-gram Embedding\n","WINDOW_SIZE = 2\n","EMBEDDING_SIZE = 2"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"de4JAr7ZT-uB"},"source":["# This is code for the skipgram generation (you don't need to understand it)\n","def skipgrams(sequences, window_size = 2, shuffle = True):\n","    couples = []\n","    for sequence in sequences:\n","        for i, wi in enumerate(sequence):\n","    \n","            window_start = max(0, i - window_size)\n","            window_end = min(len(sequence), i + window_size + 1)\n","            for j in range(window_start, window_end):\n","                if j != i:\n","                    wj = sequence[j]\n","                    couples.append([wi, wj])\n","\n","    if shuffle:\n","        seed = random.randint(0, 10e6)\n","        random.shuffle(couples)\n","\n","    return torch.Tensor(couples)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9Z7HpL2T-uB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606400999200,"user_tz":-60,"elapsed":497,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"e9f3be41-6c77-4399-af00-ebc2d63e1dfe"},"source":["# We print the path from text to skip-grams so you can see the process\n","\n","animal_vocab = SimpleVocabulary(animal_corpus)\n","\n","test_sequence = 'the cat stood on the mat'\n","print('The sentence:',test_sequence)\n","\n","tokenized_sequence = simple_tokenizer(test_sequence)\n","print('The sentence split into tokens:',tokenized_sequence)\n","\n","indexed_sequence = [animal_vocab.get_index(token) for token in tokenized_sequence]\n","print('The indexed sentence:', indexed_sequence)\n","\n","skipgrams_sequence = skipgrams([indexed_sequence], shuffle=False).int()\n","print('All generated skip-gram pairs:', skipgrams_sequence)\n","\n","skipgrams_sequence_words = [(animal_vocab.get_token(x[0]), animal_vocab.get_token(x[1])) for x in skipgrams_sequence]\n","print('The skip-gram pairs as text:', skipgrams_sequence_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The sentence: the cat stood on the mat\n","The sentence split into tokens: ['the', 'cat', 'stood', 'on', 'the', 'mat']\n","The indexed sentence: [0, 14, 3, 9, 0, 16]\n","All generated skip-gram pairs: tensor([[ 0, 14],\n","        [ 0,  3],\n","        [14,  0],\n","        [14,  3],\n","        [14,  9],\n","        [ 3,  0],\n","        [ 3, 14],\n","        [ 3,  9],\n","        [ 3,  0],\n","        [ 9, 14],\n","        [ 9,  3],\n","        [ 9,  0],\n","        [ 9, 16],\n","        [ 0,  3],\n","        [ 0,  9],\n","        [ 0, 16],\n","        [16,  9],\n","        [16,  0]], dtype=torch.int32)\n","The skip-gram pairs as text: [('the', 'cat'), ('the', 'stood'), ('cat', 'the'), ('cat', 'stood'), ('cat', 'on'), ('stood', 'the'), ('stood', 'cat'), ('stood', 'on'), ('stood', 'the'), ('on', 'cat'), ('on', 'stood'), ('on', 'the'), ('on', 'mat'), ('the', 'stood'), ('the', 'on'), ('the', 'mat'), ('mat', 'on'), ('mat', 'the')]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cU6c70uHT-uB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606401001961,"user_tz":-60,"elapsed":508,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"16dae363-3dd1-4e23-c1fe-7fe4c7ab619f"},"source":["# We create the actual skip-gram pairs for the entire \"cat on the mat\"-dataset\n","\n","animal_vocab = SimpleVocabulary(animal_corpus)\n","indexed_text = [[animal_vocab.get_index(token) for token in simple_tokenizer(text)] for text in animal_corpus]\n","pairs = skipgrams(indexed_text, WINDOW_SIZE)\n","print(f'There are {len(pairs)} pairs of skip-grams created from the \"cat in the mat\" dataset.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 2304 pairs of skip-grams created from the \"cat in the mat\" dataset.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_NE4BvbVT-uB"},"source":["For creating the skipgram embedding we will not use nn.Sequential as we usually do when creating neural networks.\n","Instead we will create a subclass of nn.Module to create our own PyTorch module.\n","This is useful when we need to create neural networks with non-linear structure, or with some quirk that cannot be implemented with existing modules.\n","In short a nn.Module needs only a single function called 'forward' that takes the input and performs the forward pass of the network using differentiable functions (most torch functions and mathematical operations supports this).\n","Any part of the module that needs to be initialized (like nn.Linear) needs to be so in the \\_\\_init\\_\\_ function of the network after the super().\\_\\_init\\_\\_() statement.\n","In this exercise you will not need to write the \\_\\_init\\_\\_ function.\n","For more information on creating networks as subclasses of nn.Module, read **[here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#nn-module)**.\n","\n","**Exercise**: Create the SkipgramEmbedder class, a neural network that first reduces the one-hot embedding of one word to a smaller embedding space and then predicts the one-hot embedding of the paired word using that. These embeddings should then be such that most groups of words are separate on the embedding space (though if two groups overlaps that is fine). Think of why the groups gets embedded the way they do."]},{"cell_type":"code","metadata":{"id":"MGEWWVGBT-uB"},"source":["# An Embedding layer used for turning int into one-hot\n","to_onehot = nn.Embedding.from_pretrained(torch.eye(len(animal_vocab))) \n","\n","class SkipgramEmbedder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.input_layer = nn.Linear(len(animal_vocab), EMBEDDING_SIZE)\n","        self.output_layer = nn.Linear(EMBEDDING_SIZE, len(animal_vocab))\n","        self.softmax = nn.Softmax(dim=1)\n","    \n","    def embed(self, context_index):\n","        \n","        # IMPLEMENT THE EMBEDDING STEP OF THE SKIP-GRAM EMBEDDER\n","        context_onehot = to_onehot(context_index)                               # Make a one-hot representation of the index\n","        embedding = self.input_layer(context_onehot)                            # Pass the one-hot representation throught the first layer to create an embedding\n","        return embedding\n","    \n","    def forward(self, context_index):\n","        \n","        # IMPLEMENT THE FORWARD PASS FOR PREDICTING TARGET WORD FROM THE CONTEXT WORD\n","        embedding = self.embed(context_index) \n","        output = self.output_layer(embedding)                                   # Pass the embedding through the second layer to create an output\n","        prediction = self.softmax(output)                                       # Pass the output through softmax \n","        return prediction\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a_OftBMAT-uB"},"source":["#### Learing the embeddings\n","Run the code below to train your embedder so it learns the embeddings. Use your previous code for the training process."]},{"cell_type":"code","metadata":{"id":"TbCOPZ24T-uB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606401011186,"user_tz":-60,"elapsed":4636,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"ab398f90-9951-4494-c038-fdaa40c09d34"},"source":["model = SkipgramEmbedder()\n","optimizer = torch.optim.RMSprop(model.parameters(), lr=0.1)\n","loss_function = nn.CrossEntropyLoss()\n","\n","#Epochs\n","TOTAL_EPOCHS = 1000\n","BATCH_SIZE = 2303\n","\n","# For each epoch\n","for epoch in range(TOTAL_EPOCHS):\n","    \n","    # A variable for containing the sum of all batch losses for this epoch\n","    epoch_loss = 0\n","    \n","    # Lower index\n","    start = 0\n","    \n","    for batch_nr, end in enumerate(range(BATCH_SIZE, len(pairs), BATCH_SIZE)):\n","        \n","        targets = torch.Tensor(pairs[start:end,0]).long()\n","        contexts = torch.Tensor(pairs[start:end,1]).long()\n","        \n","        start = end\n","        \n","        # Predict for context word in the batch what the target word should be\n","        prediction = model.forward(contexts)\n","\n","        # Calculate the loss of the prediction by comparing to the actual target word\n","        loss = loss_function(prediction, targets)\n","\n","        # Backpropogate the loss through the network to find the gradients of all parameters\n","        loss.backward()\n","\n","        # Update the parameters along their gradients\n","        optimizer.step()\n","       \n","        # Clear stored gradient values\n","        optimizer.zero_grad()\n","        \n","        # Add the loss to the total epoch loss (item() turns a PyTorch scalar into a normal Python datatype)\n","        epoch_loss += loss\n","\n","        #Print the epoch, batch, and loss\n","        print(\n","            f'\\rEpoch {epoch+1} [{batch_nr+1}/{len(targets)/BATCH_SIZE}] - Loss: {loss}',\n","            end=''\n","        )\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1000 [1/1.0] - Loss: 2.642103910446167"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8UjCiDMFT-uB"},"source":["#### Visualizing the embeddings\n","You don't need to understand the code below, instead we will explain it here.\n","Below we run each individual word in the \"cat on the mat\" dataset throgh the SkimgramEmbedder in order to get its embedding. Then we use [Prinipal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) in order to reduce the embedding dimensionality to 2 (if you have changed the dimensionality to anything larger) and then we print that as a graph where each group of words have been given the same color in order to display how they are grouped. Ideally the training should have converged towards clear groupings of similar words. If it hasn't try running the training again (it shouldn't take that many tries to get an embedding that clearly separates most groups of words). If you cannot get an embedding that separates all groups, don't worry about it so long as most groups are separated."]},{"cell_type":"code","metadata":{"id":"0jcefpqyT-uB","colab":{"base_uri":"https://localhost:8080/","height":940},"executionInfo":{"status":"ok","timestamp":1606401471702,"user_tz":-60,"elapsed":1184,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"c4f4f389-099e-419a-9421-28bfd59acf23"},"source":["%matplotlib inline\n","from sklearn.decomposition import PCA\n","from matplotlib import pyplot as plt\n","\n","\n","category_colors = {'animals' : 'green', \n","                   'actions' : 'blue',\n","                   'furniture' : 'yellow'}\n","\n","colors = []\n","for i in range(vocabulary_size):\n","    colors.append('red')\n","    \n","for word in animal_vocab.index2token:\n","    index = animal_vocab.get_index(word)\n","    if word in animals:\n","        colors[index] = category_colors['animals']\n","    elif word in actions:\n","        colors[index] = category_colors['actions']\n","    elif word in furniture:\n","        colors[index] = category_colors['furniture']\n","\n","# Plots embeddings using PCA (you don't need to understand it)\n","def plot_embeddings_after_pca(vectors):  \n","        \"\"\"\n","        Perform PCA and plot the resulting 2 components on X and Y axis\n","        Args:\n","          embedding_weights - the set of vectors to \n","        \"\"\"\n","        pca = PCA(n_components=2)\n","        vectors_pca = pca.fit_transform(vectors)\n","        plt.figure(figsize=(5,5))\n","        \n","        # We do not draw the first element, which is the 'Out-of-Vocabulary' token\n","        plt.scatter(vectors_pca[:,0], vectors_pca[:,1], c=colors, s=100, alpha=0.3);\n","        plt.title('Embeddings after PCA')\n","        legend_elements = [\n","                    plt.Line2D([0], [0], marker='o', color=category_colors['animals'], label='Animals'),\n","                    plt.Line2D([0], [0], marker='o', color=category_colors['actions'], label='Actions'),\n","                    plt.Line2D([0], [0], marker='o', color=category_colors['furniture'], label='Furniture'),\n","                    plt.Line2D([0], [0], marker='o', color='red', label='Other'),\n","                  ]\n","\n","        # Create the figure\n","        plt.legend(handles=legend_elements);\n","\n","# Extracting the embeddings for every word in the vocabulary, detaching them from PyTorch, and turning them into numpy\n","numpy_word_vectors = model.embed(torch.Tensor(range(vocabulary_size)).long()).detach().numpy()\n","\n","print('The embeddings if all words in the vocabulary')\n","print(numpy_word_vectors)\n","\n","# Display the most significant axes of the word vectors using PCA\n","plot_embeddings_after_pca(numpy_word_vectors)\n","\n","\n","# Check what the 'other' category holds (the words 'on' and 'the')\n","plt.figure()\n","plt.scatter(numpy_word_vectors[:,1], numpy_word_vectors[:,0])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The embeddings if all words in the vocabulary\n","[[ 0.92594254 -7.795179  ]\n"," [ 1.9051583   8.135748  ]\n"," [ 0.45397973 -0.07449305]\n"," [ 1.92826     8.187     ]\n"," [ 0.49560046 -0.07455316]\n"," [ 1.9303874   8.185715  ]\n"," [ 0.4541508  -0.07448563]\n"," [ 0.49562228 -0.07452554]\n"," [ 0.49556577 -0.07455528]\n"," [ 2.1424937   9.501736  ]\n"," [ 0.45409203 -0.07451984]\n"," [ 0.45399213 -0.07452455]\n"," [ 0.45394957 -0.074494  ]\n"," [ 0.45398962 -0.07448977]\n"," [ 0.45408428 -0.07451016]\n"," [ 0.45395744 -0.07449472]\n"," [ 0.49571848 -0.07454461]\n"," [ 1.9711449   8.036885  ]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.collections.PathCollection at 0x7f1c6e9c2b38>"]},"metadata":{"tags":[]},"execution_count":31},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 360x360 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4zawpz_c-Tcu","executionInfo":{"status":"ok","timestamp":1606401219778,"user_tz":-60,"elapsed":1082,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"bd7b2656-98d1-403d-f3c0-652838f55885"},"source":["vocab[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"EceZkkDAT-uB"},"source":["## Bonus task: Using pretrained embeddings USE THE OTHER DOC FOR THIS\n","\n","A problem with training word embeddings is that you would need to have a dataset containing all words you want to embed in many different sentences.\n","This adds up to needing really large datasets (the entirety of Wikipedia for example) to create good embeddings.\n","This is unfeasible for most people as it requires huge amounts of computational power and the ability to download and store huge datasets.\n","Not to mention that if everyone did this it would waste a lot of energy for everyone to do this.\n","Instead a lot of embeddings created by people access to large computational resources have been made publicly available.\n","Since it takes a lot of resources to train an embedding, but barely anything to use them this is an efficient way to get access to good word embeddings.\n","\n","We will use one such pretrained embedding in order to actually train a network to make predictions on the AG News dataset (coming soon)."]},{"cell_type":"code","metadata":{"id":"cj4zBxhfT-uB"},"source":["from gensim.models import KeyedVectors\n","\n","# Load embeddings from the pre-trained file\n","import gensim.downloader as api\n","wv = api.load('word2vec-google-news-300')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eVw6XoPiT-uB","colab":{"base_uri":"https://localhost:8080/","height":163},"executionInfo":{"status":"error","timestamp":1606898619427,"user_tz":-60,"elapsed":808,"user":{"displayName":"Pablo Santana González","photoUrl":"","userId":"08364207993842942035"}},"outputId":"9d170899-5456-482d-c8d8-c7b558d6902c"},"source":["print(wv['horse'])"],"execution_count":15,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-bfcd609b7323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'horse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'wv' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"pidr5Z7TT-uC"},"source":["Word embeddings contain mathematical representations of the underlying meaning of words. As every embedding is in the same dimensions, you can utilize linear algebra to look inside how the model thinks.\n","\n","Try playing around with a few words and methods to investigate how the model views language."]},{"cell_type":"code","metadata":{"id":"eSRrInaQT-uC","outputId":"d796110a-ee40-4dcd-d02a-cb4fa51c4b9f"},"source":["#king = wv.most_similar(\"king\")\n","dist = wv.similarity(\"king\",\"dog\")\n","#print(king)\n","print(dist)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.12812497\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2GkZ5wyBT-uC"},"source":["There is a famous example of embedding maths that goes \"king\" - \"man\" + \"woman\" = \"queen\". This example, and embedding maths in general, utilize the fact that embeddings are just vectors in a high dimenionsal vector space, and certain features of language tend to exist along certain axises and cluster together with similar words. We would thus expect king to have a presence in the \"royalty\"-axis, and in the \"man\"-axis. Removing man and adding woman should thus result in \"royalty\" + \"woman\", in other words a queen. See if you can find any other examples where semantic mathematics work, or an example where it clearly doesn't work. Do the results make sense to you?"]},{"cell_type":"code","metadata":{"id":"w9h5Q19YT-uC"},"source":["print(wv.similar_by_vector(wv['king']))\n","print(\"\")\n","similars = wv.similar_by_vector(wv['king']-wv['man']+wv['woman']-wv['boy']+wv['girl'])\n","print(similars)\n","\n","similars[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lfXzg-izT-uC"},"source":["You might be interested in investigating the vocabulary of your model, or the embedding of a specific word"]},{"cell_type":"code","metadata":{"id":"zsI0OtcQT-uC","outputId":"de141b38-dd90-4f4b-d6da-08b5a9001ec7"},"source":["for i, word in enumerate(wv.vocab):\n","    if i == 10:\n","        break\n","    print(word)\n","print(wv.get_vector(word))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["</s>\n","in\n","for\n","that\n","is\n","on\n","##\n","The\n","with\n","said\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([ 2.60009766e-02, -1.89208984e-03,  1.85546875e-01, -5.17578125e-02,\n","        5.12695312e-03, -1.09863281e-01, -8.17871094e-03, -8.83789062e-02,\n","        9.66796875e-02,  4.83398438e-02,  1.10473633e-02, -3.63281250e-01,\n","        8.20312500e-02, -2.12402344e-02,  1.58203125e-01,  4.41894531e-02,\n","       -1.17797852e-02,  2.12890625e-01, -5.73730469e-02,  5.66406250e-02,\n","       -1.07421875e-01,  1.85546875e-01,  7.71484375e-02,  1.44958496e-04,\n","        1.52343750e-01, -6.54296875e-02, -1.52343750e-01,  2.25585938e-01,\n","        8.10546875e-02,  8.88671875e-02,  7.32421875e-02, -1.03515625e-01,\n","       -6.68945312e-02,  1.76757812e-01,  2.12890625e-01,  1.40625000e-01,\n","       -3.41796875e-02,  1.78222656e-02,  5.95703125e-02,  2.86102295e-04,\n","        5.88378906e-02,  9.27734375e-03,  1.66992188e-01, -2.70080566e-03,\n","        1.15722656e-01,  1.04492188e-01,  5.37109375e-02,  1.85546875e-02,\n","        1.06445312e-01,  5.05371094e-02, -1.64794922e-02, -1.27929688e-01,\n","        2.16796875e-01,  5.15136719e-02,  4.78515625e-02,  1.52343750e-01,\n","        1.71875000e-01,  7.86132812e-02, -5.88378906e-02, -4.29687500e-02,\n","       -7.27539062e-02,  1.81640625e-01, -8.05664062e-02, -1.54296875e-01,\n","       -1.16699219e-01,  8.44726562e-02, -6.17675781e-02, -4.51660156e-02,\n","        9.21630859e-03,  1.33789062e-01,  1.92871094e-02,  6.44531250e-02,\n","        1.08886719e-01,  1.58203125e-01, -2.35595703e-02,  1.23535156e-01,\n","        1.69921875e-01,  3.49121094e-02,  1.29882812e-01,  2.65625000e-01,\n","        1.93359375e-01, -8.83789062e-02,  8.49609375e-02, -2.96630859e-02,\n","        5.76171875e-02,  2.51464844e-02, -1.01562500e-01,  1.99218750e-01,\n","        1.04492188e-01, -2.42919922e-02,  2.01416016e-02, -3.51562500e-02,\n","        6.64062500e-02, -6.20117188e-02,  2.90527344e-02, -9.81445312e-02,\n","       -1.81640625e-01,  2.14843750e-01, -5.76171875e-02, -4.51660156e-02,\n","        4.49218750e-02, -1.95312500e-02, -2.08984375e-01,  1.19628906e-01,\n","       -9.03320312e-02,  5.07812500e-02,  9.03320312e-03, -9.76562500e-02,\n","       -7.86132812e-02, -1.36718750e-01, -1.13769531e-01, -5.64575195e-03,\n","       -4.07714844e-02, -2.05993652e-03, -5.66406250e-02,  3.64685059e-03,\n","        8.30078125e-02, -7.08007812e-02,  2.63671875e-01,  1.24511719e-01,\n","       -1.61132812e-02,  9.13085938e-02, -2.39257812e-01, -1.04980469e-02,\n","       -6.78710938e-02,  1.40625000e-01,  2.34375000e-01, -6.39648438e-02,\n","        1.95312500e-01,  5.02929688e-02, -1.25000000e-01,  2.06298828e-02,\n","       -1.19140625e-01, -1.17187500e-01, -9.01222229e-05,  3.68652344e-02,\n","        1.46484375e-01,  2.47802734e-02, -1.49414062e-01,  3.03649902e-03,\n","       -3.10058594e-02,  1.06933594e-01,  2.55859375e-01, -6.00585938e-02,\n","       -2.07031250e-01,  1.58203125e-01, -2.15820312e-01, -1.84570312e-01,\n","       -1.72851562e-01,  7.99560547e-03, -3.03955078e-02,  9.81445312e-02,\n","        4.66918945e-03,  2.57812500e-01,  1.06933594e-01,  1.26953125e-01,\n","        6.34765625e-02, -1.30859375e-01,  6.54296875e-02, -9.91210938e-02,\n","        5.90820312e-02, -3.71093750e-02,  1.01074219e-01,  1.53320312e-01,\n","       -1.53320312e-01, -7.56835938e-02,  5.85937500e-02, -5.05371094e-02,\n","        2.08007812e-01,  4.85839844e-02, -9.42382812e-02, -9.71679688e-02,\n","       -1.23046875e-01, -1.97265625e-01, -1.76757812e-01, -1.11328125e-01,\n","        1.11328125e-01, -5.88378906e-02,  2.27539062e-01,  4.00390625e-02,\n","        1.24511719e-01,  1.47460938e-01,  1.81884766e-02,  4.05273438e-02,\n","        1.69921875e-01,  1.13769531e-01, -2.24609375e-02,  6.73828125e-02,\n","        8.59375000e-02,  6.73828125e-02,  2.06298828e-02,  4.78515625e-02,\n","        1.84326172e-02,  2.05078125e-01, -4.68750000e-02,  2.00195312e-01,\n","       -1.56250000e-02, -1.40625000e-01,  1.09863281e-02, -1.73828125e-01,\n","        4.85839844e-02, -1.58203125e-01, -1.04492188e-01,  3.63769531e-02,\n","        3.01513672e-02,  1.27929688e-01, -1.14257812e-01,  1.41601562e-01,\n","        2.34375000e-01, -8.98437500e-02, -1.02996826e-03, -1.50390625e-01,\n","        1.79687500e-01,  1.35742188e-01, -2.08007812e-01, -1.27563477e-02,\n","        1.75781250e-01, -1.39648438e-01, -2.03125000e-01, -3.00292969e-02,\n","       -2.78320312e-02, -6.50024414e-03,  1.26953125e-01, -1.49414062e-01,\n","        1.46484375e-01, -8.42285156e-03,  1.12304688e-01,  1.66015625e-01,\n","       -1.57470703e-02,  1.23046875e-01,  7.22656250e-02, -4.37011719e-02,\n","       -7.56835938e-02, -9.03320312e-02,  1.01562500e-01, -1.44531250e-01,\n","       -4.00390625e-02, -1.26953125e-02,  2.66113281e-02, -7.81250000e-02,\n","        3.56445312e-02,  3.49121094e-02,  1.79687500e-01, -1.38671875e-01,\n","        2.80761719e-02, -2.86865234e-02,  6.78710938e-02,  7.03125000e-02,\n","        9.57031250e-02,  5.00488281e-02, -2.20947266e-02, -3.00781250e-01,\n","        1.14257812e-01,  7.51953125e-02,  1.26342773e-02,  1.32812500e-01,\n","        2.52685547e-02,  3.63769531e-02, -2.81982422e-02, -1.36718750e-01,\n","        1.79687500e-01, -9.27734375e-02,  8.49609375e-02,  1.32812500e-01,\n","        3.97949219e-02,  4.29687500e-01, -1.87988281e-02, -1.47460938e-01,\n","        6.10351562e-02,  9.03320312e-02,  8.69140625e-02, -6.88476562e-02,\n","        1.10839844e-01,  9.81445312e-02,  1.50390625e-01,  1.61132812e-01,\n","       -8.05664062e-02, -1.74804688e-01, -3.32031250e-02, -1.28906250e-01,\n","        1.22558594e-01, -1.44653320e-02, -1.63085938e-01, -3.58886719e-02,\n","        2.78320312e-02, -6.34765625e-02, -7.91015625e-02, -1.14746094e-01,\n","        1.84326172e-02,  2.91748047e-02, -3.00781250e-01, -4.58984375e-02,\n","       -1.74804688e-01,  2.33398438e-01,  2.25830078e-02,  1.10351562e-01,\n","       -1.03515625e-01, -1.21582031e-01,  2.21679688e-01, -2.19726562e-02],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"ghCR3vgGT-uC"},"source":["# FUNCTION STORAGE\n","\n","def to_onehot(list_of_numbers):\n","    if list_of_numbers.shape == torch.Size([]):\n","        return torch.tensor([np.eye(4)[list_of_numbers]])\n","    else:\n","        generator = (i for i in list_of_numbers)\n","        return torch.tensor([np.eye(4)[i] for i in generator])\n","\n","\n","def to_float(tensor):\n","    generator = (i for i in tensor)\n","    return torch.tensor([torch.float(i) for i in generator])\n"],"execution_count":null,"outputs":[]}]}